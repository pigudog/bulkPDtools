```{r}
## 可以先尝试使用merge.prenorm，去批次前的诊断模型说服力最好！！
## 如果roc不理想，则使用merge.normalzie！
## 先试下面
#rt=read.table('merge.preNorm.txt', header=T, sep="\t", check.names=F, row.names=1)
## 再试下面
rt=read.table('data/merge.normalize.txt', header=T, sep="\t", check.names=F, row.names=1)
```

```{r}
load('data/GSE138518.Rdata')
pdata1=pdata
group_list1=c(rep('CT',3),rep('PC',3))

load('data/GSE155489.Rdata')
pdata2=pdata
group_list2=c(rep('CT',4),rep('PC',4))

load('data/GSE193123.Rdata')
pdata3=pdata
group_list3=c(rep('CT',3),rep('PC',3))

# GSE168404.txt
load('data/GSE168404.Rdata')
pdata4=pdata
group_list4=c(rep('PC',5),rep('CT',5))

intersect_gene <- read.table("data/mito_hub.txt")
intersect_gene <- intersect_gene$V1
```

# 构建训练集和测试集
```{r}
## train
a = grep('GSE138518',colnames(rt))
b = grep('GSE155489',colnames(rt))
c = grep('GSE193123',colnames(rt))
d= grep('GSE168404',colnames(rt))
# train=rt[,c(a,b,c)]
train=rt[,c(a,b,c,d)]
# group_list_train=c(group_list1,group_list2,group_list3)
group_list_train=c(group_list1,group_list2,group_list3,group_list4)
group_list_train=factor(group_list_train,levels=c('CT','PC'))
train=train[intersect_gene,]
train=as.data.frame(t(train))
train <- train[,-4]
train <- train[,-31]
save(train,group_list_train,file ='train.Rdata')
```

```{r}
test=rt[,d]
group_list_test= group_list4
group_list_test=factor(group_list_test,levels=c('CT','PC'))
test=test[intersect_gene,]
test=as.data.frame(t(test))
test <- test[,-4]
test <- test[,-31]
save(test,group_list_test,file ='test.Rdata')
```

# SVM-REF
```{r}
library(caret)
control <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
set.seed(1314)
# 执行SVM-RFE算法
results <- rfe(train, 
               group_list_train,
               rfeControl = control,
               method = "svmRadial")

# 结果分析
print(results)
```

```{r}
# 列出选择的变量集
rfe_gene=predictors(results)[1:8]
rfe_gene
```

# Random Forest
```{r}
library(randomForest)
set.seed(521)
rf=randomForest(x = train,y = group_list_train,ntree = 1000)
#rf=randomForest(group~., data=data, ntree=500)

plot(rf, main="Random forest", lwd=2)
```

```{r}
#找出误差最小的点
optionTrees=which.min(rf$err.rate[,1])
optionTrees
```

```{r}
set.seed(1314)
rf2=randomForest(x = train,y = group_list_train, ntree=optionTrees)
#查看基因的重要性
importance=importance(x=rf2)
# dev.off()
#绘制基因的重要性图
varImpPlot(rf2, main="")
```

```{r}
#挑选疾病特征基因
rfGenes=importance[order(importance[,"MeanDecreaseGini"], decreasing = TRUE),]
# 可以调节！
rfGenes=names(rfGenes[rfGenes>0.1])     #挑选重要性评分大于0的基因
rfGenes
```
# LASSO
```{r}
library(dplyr)
library(glmnet)
x=as.matrix(train)
y=unlist(group_list_train)
set.seed(100000)
fit <- glmnet(x,y,alpha=1,family="binomial")
# 手动保存
plot(fit,xvar="lambda",label=F)
```
```{r}
cvfit <- cv.glmnet(x,y,alpha=1,family="binomial",nfolds = 10)
plot(cvfit)
```
```{r}
coef =coef(fit,s = cvfit$lambda.min)
index = which(coef !=0)
actCoef = coef[index] 
lassoGene = row.names(coef)[index] 
geneCoef = cbind(Gene=lassoGene,Coef=actCoef) 
geneCoef   #查看模型的相关系数
```
```{r}
lasso_genes=geneCoef[,1][2:length(lassoGene)]
lasso_genes
```

```{r}
intersect_gene <- intersect(lasso_genes,rfe_gene)
intersect_gene <- intersect(intersect_gene,rfGenes)
intersect_gene
```
```{r}
colnames(train)
```
```{r}
library(PDtools)
x= list(SVM_RFE=rfe_gene, #列表Deseq2
         RandomForest=rfGenes,#列表edgeR
         LASSO=lasso_genes#limma
        )
PDtools::draw_venn(x,"venn plot",color = c("#2874C5","#f87669","#e6b707"))
```


```{r}
color=c("#2874C5","#f87669","#e6b707")
library(VennDiagram)
geneList <- list()
geneList[["SVM-RFE"]]=rfe_gene
geneList[["RandomForest"]]=rfGenes
geneList[["LASSO"]]=lasso_genes
venn.plot=venn.diagram(geneList, filename=NULL, fill=color, scaled=FALSE, cat.col=color, cat.cex = 1.5)
pdf(file="Figure4/venn.pdf", width=5, height=5)
grid.draw(venn.plot)
dev.off()
```
```{r}
write.table(rfe_gene, file="data/rfe_gene.txt", sep="\t", quote=F, col.names=F, row.names=F)
write.table(rfGenes, file="data/rfGenes.txt", sep="\t", quote=F, col.names=F, row.names=F)
write.table(lasso_genes, file="data/lasso_genes.txt", sep="\t", quote=F, col.names=F, row.names=F)
write.table(intersect_gene, file="data/intersect_gene.txt", sep="\t", quote=F, col.names=F, row.names=F)
```

```{r}
library(pROC)
for (i in intersect_gene) {
  pdf(paste0("Figure4/",i,'.pdf'),width = 4,height = 4)
  plot.roc(y,as.numeric(train[,i]),print.auc=T)
  dev.off()
}
```

```{r}
#????ͼ?ε???ɫ
bioCol=rainbow(length(intersect_gene), s=0.9, v=0.9)

#?Ժ??Ļ???????ѭ????????ROC????
aucText=c()
rocSigGenes=c()
k=0
y=unlist(group_list_train)
i = intersect_gene[1]
for(i in intersect_gene){
	k=k+1
	#????ROC????
	roc1=roc(y, as.numeric(train[,i]),print.auc=T)     #?õ?ROC???ߵĲ???
	if(k==1){
		pdf(file="./Figure4/ROC.genes.pdf", width=6, height=5.5)
		plot(roc1, print.auc=F, col=bioCol[k], legacy.axes=T, main="")
		aucText=c(aucText, paste0(x,", AUC=",sprintf("%.3f",roc1$auc[1])))
	}else{
		plot(roc1, print.auc=F, col=bioCol[k], legacy.axes=T, main="", add=TRUE)
		aucText=c(aucText, paste0(x,", AUC=",sprintf("%.3f",roc1$auc[1])))
	}
}
#????ͼ????չʾROC?????µ?????
legend("bottomright", aucText, lwd=2, bty="n", col=bioCol[1:(ncol(rt)-1)])
dev.off()
```
# XGboost建立模型
```{r}
## 可以先尝试使用merge.prenorm，去批次前的诊断模型说服力最好！！
## 如果roc不理想，则使用merge.normalzie！
## 先试下面
# rt=read.table('merge.preNorm.txt', header=T, sep="\t", check.names=F, row.names=1)
## 再试下面
rt=read.table('./data/merge.normalize.txt', header=T, sep="\t", check.names=F, row.names=1)

rt=rt[intersect_gene,]

## 训练集
X_train=t(rt[,c(a,b,c)])
group_list_train=c(group_list1,group_list2,group_list3)
group_list_train=factor(group_list_train,levels=c('CT','PC'))
Y_train=ifelse(group_list_train=='CT',0,1)

## 验证集
X_test=t(rt[,d])
Y_test=ifelse(group_list_test == 'CT',0,1)
```

```{r}
library(xgboost)
library(rBayesianOptimization)
set.seed(1314)
dtrain <- xgb.DMatrix(data = X_train, label = Y_train)
cv_folds <- KFold(Y_train, nfolds = 10, stratified = TRUE, seed = 0)
# 交叉验证，贝叶斯优化。以下基本不需修改，nround = 10可修改
xgb_cv_bayes <- function(eta, max.depth, min_child_weight, subsample) {
  cv <- xgb.cv(
    params = list(
      booster = "gbtree",
      eta = eta,
      max_depth = max.depth,
      min_child_weight = min_child_weight,
      subsample = subsample,
      colsample_bytree = 0.6,
      lambda = 1,
      alpha = 0,
      objective = "binary:logistic",
      eval_metric = "auc"
    ),
    data = dtrain,
    #减少循环次数以作示例，可增大（到100）
    nround = 10,
    folds = cv_folds,
    prediction = TRUE,
    showsd = TRUE,
    early.stop.round = 5,
    maximize = TRUE,
    verbose = 0
  )
  list(
    Score = cv$evaluation_log[, max(test_auc_mean)], Pred = cv$pred
  )
}

## 参数调优
## 生成梯度的参数，基本不需修改
set.seed(1314)
OPT_Res <- BayesianOptimization(
  xgb_cv_bayes,
  bounds = list(
    eta = c(0.01L, 0.05L, 0.1L, 0.3L),
    max.depth = c(6L, 8L, 12L),
    min_child_weight = c(1L, 10L),
    subsample = c(0.5, 0.8, 1)),
  init_grid_dt = NULL,
  init_points = 10,
  # 减少迭代次数以作示例，可增大
  n_iter = 10,
  acq = "ucb",
  kappa = 2.576,
  eps = 0.0,
  verbose = TRUE
)

# 应用参数
params <- list(
  "eta" = unname(OPT_Res$Best_Par["eta"]),
  "max_depth" = unname(OPT_Res$Best_Par["max.depth"]),
  "colsample_bytree" = 1,
  "min_child_weight" = unname(OPT_Res$Best_Par["min_child_weight"]),
  "subsample"= unname(OPT_Res$Best_Par["subsample"]),
  "objective"="binary:logistic",
  "gamma" = 1,
  "lambda" = 1,
  "alpha" = 0,
  "max_delta_step" = 0,
  "colsample_bylevel" = 1,
  "eval_metric"= "auc",
  "set.seed" = 1314
)


watchlist <- list("train" = dtrain)
nround = 20
xgb.model <- xgb.train(params, dtrain, nround, watchlist)
```

```{r}
dtest <- xgb.DMatrix(data = X_test)
# 训练集
XGB_train_Predictions <- predict(object = xgb.model, newdata = dtrain, type = 'prob')
# 验证集
XGB_Predictions <- predict(object = xgb.model, newdata = dtest, type = 'prob')

#大致看一下
pROC::plot.roc(as.factor(Y_train),XGB_train_Predictions,print.auc=T)
```
```{r}
pROC::plot.roc(as.factor(Y_test),XGB_Predictions,print.auc=T)
```

